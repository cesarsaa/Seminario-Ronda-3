---
title: "Análisis de Imágenes"
author: "Angie Rodríguez Duque & César Saavedra Vanegas"
date: "Diciembre 04 de 2020"
output:
  ioslides_presentation:
    widescreen: yes
    smaller: yes
    transition: slower
  beamer_presentation: default
---

## Introducción
<div style="text-align: justify">


El análisis de imágenes se utiliza como una herramienta fundamental para reconocer, diferenciar y cuantificar diversos tipos de imágenes, incluidas imágenes en escala de grises y en color, imágenes multiespectrales para unos pocos canales espectrales discretos o bandas de ondas (normalmente menos de 10) e imágenes hiperespectrales con una secuencia de bandas de ondas contiguas que cubren una región espectral específica (por ejemplo, visible e infrarrojo cercano).

Es importante señalar que el análisis de imágenes es parte de un campo más amplio conocido como **procesamiento de imágenes**, donde el principal objetivo es mejorar la calidad visual de una imagen y/o extraer información o características útiles. El análisis se basa en diferentes propiedades de la imagen como el color, el brillo, la morfología de los objetos y la textura.

## Introducción

<div style="text-align: justify">

Las acciones de procesamiento de imágenes se pueden agrupar en tres subáreas (Prats-Montalbán et al. 2011):

+ **Compresión de imagen** Reduce los requisitos de memoria al eliminar la redundancia presente en la imagen, es decir, la información de la imagen que no es perceptible para el ojo humano.

- **Preprocesamiento de imágenes** Consiste en mejorar la calidad visual de la imagen reduciendo el ruido, la calibración y estandarización de píxeles, mejorando la detección de bordes y haciendo que el análisis de la imagen sea más confiable en base a criterios objetivos y bien establecidos. El término preprocesamiento de imágenes, en general, se refiere a todas las manipulaciones en una imagen, cada una de las cuales produce una nueva imagen.

+ **Análisis de imágenes** Generalmente devuelve valores numéricos y / o información gráfica sobre las características de la imagen que son adecuadas para la clasificación, detección de defectos o predicción de algunas de las propiedades de calidad del objeto de la imagen. El término análisis de imágenes se utiliza cuando el resultado es un número o una decisión, no una imagen.

<div>

##
### **El análisis de imágenes como problema estadístico**

<div style="text-align: justify">

El análisis de imágenes visto como un análisis estadístico es un campo próspero que vió el surgimiento de varios avances estadísticos importantes, incluido, por ejemplo: **El muestreador de Gibbs**. Además, este campo ha adoptado predominantemente una **perspectiva bayesiana** tanto porque esto era algo natural de hacer como porque el poder analítico de este enfoque era mayor que con otros métodos. 

La razón de esta aparente paradoja es que, mientras que los píxeles suelen ser deterministas objetos, la complejidad y el tamaño de las imágenes requieren que uno represente esos píxeles como la salida aleatoria de una distribución gobernada por un objeto de dimensión mucho más pequeña. Por ejemplo, este es el caso de la visión por computadora, donde los objetos específicos deben extraerse de un fondo mucho más rico (o más ruidoso).

<div>

# Dependencia espacial

## Rejillas y Lattices
<div style="text-align: justify">
Una imagen (en el sentido de una imagen generada por computadora) es un caso especial de un lattice, en el sentido de que es un objeto aleatorio cuyos elementos están indexados por la ubicación de los píxeles y, por lo tanto, están relacionados por la proximidad geográfica de esas ubicaciones. 

**Lattice:** Es un objeto matemático multidimensional en el que se puede definir una relación de vecindad. 

<center>
![](1.png "Red neuronal"){height=200px}
<div style="text-align: justify">
Las ubicaciones pertenecen a un conjunto discreto y son seleccionadas por el investigador.
Ejemplos: tasas de mortalidad por departamentos, producción por área, colores de píxeles en interpretación de imágenes satélite.

<div>

## Modelo de Ising

<div style="text-align: justify">

Si los píxeles de la imagen $x$ bajo estudio sólo pueden tomar dos colores (blanco y negro), entonces $x$ es binario.De esta manera, nos referimos a cada pixel $x_{i}$ como:

+ **Primer plano:** Si $x_{i}=1$ (Negro)

- **Fondo:** $x_{i}=0$ (Blanco)

Además, la distribución condicional de un pixel es Bernoulli con el parámetro de probabilidad correspondiente dependiendo de los otros píxeles. Un paso de simplificación es asumir que es una función del número de píxeles vecinos negros, utilizando, por ejemplo, un enlace logit como $(j=0,1)$

$$\pi(x_{i}=j|x_{-i})\varpropto exp(\beta n_{i,j}), \hspace{2cm} \beta >0$$

Donde:

$n_{i,j}=\sum_{\ell \in n(i)}:$ Es el número de vecinos de $x_{i}$ con color j

<div>

## Modelo de Ising

<div style="text-align: justify">

El modelo de Ising se define a través de los siguientes condicionales:


$$\pi(x_{i}=1|x_{-i})=\displaystyle{\frac{exp(\beta n_{i,1})}{exp(\beta n_{i,0})+exp(\beta n_{i,1})}}$$

y la distribución conjunta por tanto satisface:

$$\pi(x)\varpropto exp\left(\beta \sum_{j\sim i} I_{x_{j}=x_{i}}\right)$$

Donde la suma se toma sobre todos los pares $(i,j)$ de vecinos.

<div>

## El modelo de Potts

<div style="text-align: justify">

Es la generalización del modelo de Ising a los casos en que la imagen tiene más de dos colores. 

Si $n_{i,g}$ denota el número de vecinos de $i \in \imath$ con color g $(1\leq g \leq G)$, esto es:

$$n_{i,g}= \sum_{j\sim i} I_{xj=g}$$
La distribución condicional completa de $x_{i}$ se elige como:

$$\pi(x_{i}=g|x_{-i})\varpropto exp(\beta n_{i,g})$$

Esta elección corresponde a un modelo de probabilidad conjunta (verdadero), el modelo de Potts, cuya densidad viene dada por:

$$\pi(x)\varpropto exp\left(\beta \sum_{j\sim i} I_{x_{j}=x_{i}}\right)$$

<div>

## Segmentación de imágenes

<div style="text-align: justify">

La estructura de los píxeles "verdaderos" se denota por $x$, mientras que la imagen observada se denota por $y$. Ambos objetos $x$ e $y$ son matrices, con cada entrada de $x$ tomando un número finito de valores y cada entrada de $y$ tomando valores reales. Por lo tanto, estamos interesados en la distribución posterior de $x$ dada $y$ proporcionada por el teorema de Bayes:

$$\pi(x|y)\varpropto f(y|x)\pi(x)$$

En esta distribución posterior:

+ $f(y|x)$: La verosimilitud describe el vínculo entre la imagen observada y la clasificación subyacente; es decir, da la distribución del ruido.

- $\pi(x)$: La apriori codifica creencias sobre las propiedades (posibles o deseadas) de la imagen subyacente.

<div>

##
### **Pasos para el procesamiento y análisis de imágenes**

<div style="text-align: justify">

El procesamiento de imágenes implica una serie de 5 operaciones para mejorar la calidad de una imagen digital con el fin de eliminar defectos como distorsión geométrica, enfoque inadecuado, ruido repetitivo, iluminación no uniforme y movimiento de la cámara. Estas son:

<center>
![](proceso.png "Red neuronal"){height=400px}

<div>

## Aplicaciones
<div class="columns-2">
<div style="text-align: justify">

+ **Medicina**
+ **Seguridad**
+ **Reconocimiento de imagenes satelitales**
<div>

<center>
![](medical.jpeg "Red neuronal"){height=200px}
<div>

<center>
![](fa.jpeg "Red neuronal"){height=200px}
<div>

<center>
![](sa.jpg "Red neuronal"){height=200px}
<div>

<div class="columns-2">
<div>

## Ejemplo de aplicación

<div style="text-align: justify">

A manera de ejemplo se construirá un clasificador binario que puede distinguir entre perros y gatos de un conjunto de imágenes, utilizando la interfaz Keras R  impulsada por el  motor de back-end TensorFlow.

<center>
![](perro_gato.gif "Red neuronal"){width=500px}
<div>

## ¿Qué es CNN?

<div style="text-align: justify">

El objetivo es experimentar con redes neuronales convolucionales (CNN) una forma de aprendizaje profundo. Las CNN son la base de los algoritmos de reconocimiento más avanzados utilizados por los principales gigantes tecnológicos. En resumen, se piensa en CNN como un algoritmo de aprendizaje automático que puede tomar una imagen de entrada, asignar importancia (pesos y sesgos aprendibles) a varios aspectos/objetos en la imagen y poder diferenciar uno del otro. CNN funciona extrayendo características de las imágenes. 

Cualquier CNN consta de lo siguiente:

1. La capa de entrada que es una imagen en escala de grises.
2. La capa de salida, que es una etiqueta binaria o de varias clases.
3. Capas ocultas que constan de capas de convolución, capas ReLU (unidad lineal rectificada), capas de agrupación y una red neuronal completamente conectada.

<div>

```{r warning=FALSE, include=F, paged.print=F}
suppressMessages(library(keras))
suppressMessages(library(EBImage))
suppressMessages(library(stringr))
suppressMessages(library(pbapply))
suppressMessages(library(magrittr))
setwd("/Users/cesar.saavedra/Documents/GitHub/Seminario-Ronda-3/train")
#setwd("C:/Users/Angie Rodríguez/Documents/GitHub/Seminario-Ronda-3/train")
```

## Ariel
```{r warning=FALSE, include=T, paged.print=F}
secondCat <- readImage("/Users/cesar.saavedra/Documents/Emprendimiento/R\ imagenes/train/cat.0.jpg")
display(secondCat)
```

## Nala
```{r warning=FALSE, include=T, paged.print=F}
firstdog <- readImage("/Users/cesar.saavedra/Documents/Emprendimiento/R\ imagenes/train/dog.0.jpg")
display(firstdog)
```

```{r warning=FALSE, include=F, paged.print=F}
# Set image size
width <- 50
height <- 50
extract_feature <- function(dir_path, width, height, labelsExist = T) {
  img_size <- width * height
  ## List images in path
  images_names <- list.files(dir_path)
  if(labelsExist){
    ## Select only cats or dogs images
    catdog <- str_extract(images_names, "^(cat|dog)")
    # Set cat == 0 and dog == 1
    key <- c("cat" = 0, "dog" = 1)
    y <- key[catdog]
  }
  print(paste("Start processing", length(images_names), "images"))
    ## This function will resize an image, turn it into greyscale
  feature_list <- pblapply(images_names, function(imgname) {
    ## Read image
    img <- readImage(file.path(dir_path, imgname))
    ## Resize image
    img_resized <- resize(img, w = width, h = height)
    ## Set to grayscale (normalized to max)
    grayimg <- channel(img_resized, "gray")
    ## Get the image as a matrix
    img_matrix <- grayimg@.Data
    ## Coerce to a vector (row-wise)
    img_vector <- as.vector(t(img_matrix))
    return(img_vector)
  })
  ## bind the list of vector into matrix
  feature_matrix <- do.call(rbind, feature_list)
  feature_matrix <- as.data.frame(feature_matrix)
  ## Set names
  names(feature_matrix) <- paste0("pixel", c(1:img_size))
  if(labelsExist){
    return(list(X = feature_matrix, y = y))
  }else{
    return(feature_matrix)
  }
}
```

```{r warning=FALSE, include=F, paged.print=F}
# Takes approx. 15min
trainData <- extract_feature("train/", width, height)
# Takes slightly less
testData <- extract_feature("train/", width, height, labelsExist = F)
```

# Procesamiento
## 
```{r warning=FALSE, include=T, paged.print=F}
# Tomando como referencia a Ariel
par(mar = rep(0, 4))
testCat <- t(matrix(as.numeric(trainData$X[2,]),
                    nrow = width, ncol = height, T))
image(t(apply(testCat, 2, rev)), col = gray.colors(12),
      axes = F)
```

```{r warning=FALSE, include=F, paged.print=F}
# Save / load
save(trainData, testData, file = "catdogData.RData")
```

#Construcción del modelo
##
```{r warning=FALSE, include=F, paged.print=F}
# Fix structure for 2d CNN
train_array <- t(trainData$X)
dim(train_array) <- c(50, 50, nrow(trainData$X), 1)
# Reorder dimensions
train_array <- aperm(train_array, c(3,1,2,4))
test_array <- t(testData)
dim(test_array) <- c(50, 50, nrow(testData), 1)
# Reorder dimensions
test_array <- aperm(test_array, c(3,1,2,4))
# Check cat again
testCat <- train_array[2,,,]
image(t(apply(testCat, 2, rev)), col = gray.colors(12),
      axes = F)
# Build CNN model
model <- keras_model_sequential() 
model %>% 
  layer_conv_2d(kernel_size = c(3, 3), filter = 32,
                activation = "relu", padding = "same",
                input_shape = c(50, 50, 1),
                data_format = "channels_last") %>%
  layer_conv_2d(kernel_size = c(3, 3), filter = 32,
                activation = "relu", padding = "valid") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_dropout(rate = 0.25) %>%
  layer_conv_2d(kernel_size = c(3, 3), filter = 64, strides = 2,
                activation = "relu", padding = "same") %>%
  layer_conv_2d(kernel_size = c(3, 3), filter = 64,
                activation = "relu", padding = "valid") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>%
  layer_dense(units = 50, activation = "relu") %>% 
  layer_dropout(rate = 0.25) %>%
  layer_dense(units = 1, activation = "sigmoid")
summary(model)
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = "adam",
  metrics = c('accuracy')
)
history <- model %>% fit(
  x = train_array, y = as.numeric(trainData$y), 
  epochs = 30, batch_size = 100, 
  validation_split = 0.2
)
```

```{r fig.height=4, fig.width=8, warning=FALSE, include=T, paged.print=FALSE}
plot(history)
```

# Clasificación final
##
```{r warning=FALSE, include=F, paged.print=F}
# Compute probabilities and predictions on test set
predictions <-  predict_classes(model, test_array)
probabilities <- predict_proba(model, test_array)
```

```{r warning=FALSE, include=T, paged.print=F}
# Visual inspection of 9 cases
set.seed(100)
random <- sample(1:nrow(testData), 9)
preds <- predictions[random,]
probs <- as.vector(round(probabilities[random,], 2))
par(mfrow = c(4, 8), mar = rep(0, 4))
for(i in 1:length(random)){
  image(t(apply(test_array[random[i],,,], 2, rev)),
        col = gray.colors(12), axes = F)
  legend("topright", legend = ifelse(preds[i] == 0, "Cat", "Dog"),
         text.col = ifelse(preds[i] == 0, 2, 4), bty = "n", text.font = 2)
  legend("topleft", legend = probs[i], bty = "n", col = "white")
}
```

## Conclusiones

<div style="text-align: justify">

+ El campo del procesamiento y análisis de imágenes ha experimentado un crecimiento exponencial y su aplicación se ha generalizado cada vez más en los últimos años. 

- El desarrollo de algoritmos nuevos y más eficaces para el procesamiento y análisis de imágenes, junto con los avances en la adquisición de imágenes, la computadora, el almacenamiento de datos e Internet, han hecho posible manejar un volumen cada vez mayor de datos de imágenes. 

+ El procesamiento de imágenes digitales se ha vuelto económico en muchos campos de investigación y en aplicaciones industriales. Si bien cada aplicación es diferente o única de las demás, a todas les preocupa la velocidad, la asequibilidad y el rendimiento o la precisión. 

- Cada vez más actividades de investigación y aplicaciones en visión artificial se centran en operaciones interactivas y en tiempo real, donde la adquisición, el procesamiento, el análisis y la toma de decisiones de imágenes se realizan casi de forma simultánea o en paralelo.

<div>

## Bibliografía

<div style="text-align: justify">

+ Marin, J. M., & Robert, C. (2007). Bayesian core: a practical approach to computational Bayesian statistics. Springer Science & Business Media.

- Marin, J. M., & Robert, C. P. (2014). Bayesian essentials with R (Vol. 48). New York: Springer.

+ Mendoza, F., & Lu, R. (2015). Basics of image analysis. In Hyperspectral imaging technology in food and agriculture (pp. 9-56). Springer, New York, NY.

- Prats-Montalbán, J. M., De Juan, A., & Ferrer, A. (2011). Multivariate image analysis: a review with applications. Chemometrics and Intelligent Laboratory Systems, 107(1), 1-23.


<div>

# ¡Gracias por tu atención!
