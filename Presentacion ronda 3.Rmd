---
title: "Análisis de Imágenes"
author: "Angie Rodríguez Duque & César Saavedra Vanegas"
date: "Diciembre 04 de 2020"
output:
  ioslides_presentation:
    widescreen: yes
    smaller: yes
    transition: slower
  beamer_presentation: default
---

## Introducción
<div style="text-align: justify">



<center>
![](4.gif "Red neuronal"){width=500px}
<div>

#
### El análisis de imágenes como problema estadístico

<div style="text-align: justify">



<div class="columns-2">
Sus partes son:

+ **Dendritas:** Constituyen el canal de entrada de la información.

- **Sinapsis:** Conexión entre neuronas.

+ **Cuerpo celular (Soma):** Es el órgano de cómputo.

- **Axón:** Corresponde al canal de salida.

<center>
![](red2.gif "Red neuronal"){height=250px}
<div class="columns-2">

# Dependencia espacial

#
### Rejillas y Lattices

<div style="text-align: justify">

<center>
![](simple_profunda.png "Red neuronal"){height=250px}

#
### Modelo de Ising

<center>
![](1.png "Red neuronal"){height=400px}
#
### El modelo de Potts

<div style="text-align: justify">

#
### Segmentación de imágenes

# Aprendizaje automático


## **¿Cómo entrenar una red neuronal?**
<div style="text-align: justify">

Se trata de un proceso iterativo de “ir y venir” por las capas de neuronas.

+ **Forwardpropagation:** Se expone la red a los datos de entrenamiento y estos cruzan toda la red neuronal para ser calculadas sus predicciones (labels). Es decir, pasar los datos de entrada a través de la red de tal manera que todas las neuronas apliquen su transformación a la información que reciben de las neuronas de la capa anterior y la envíen a las neuronas de la capa siguiente. Cuando los datos hayan cruzado todas las capas, y todas sus neuronas han realizado sus cálculos, se llegará a la capa final con un resultado de predicción de la label para aquellos ejemplos de entrada. 

- **Función de Loss:** Se usa para estimar el error, para comparar y medir qué tan bueno/malo fue el resultado de la predicción en relación con el resultado correcto. Idealmente, queremos que nuestro coste sea cero, es decir, sin divergencia entre valor estimado y el esperado. Por eso a medida que se entrena el modelo se irán ajustando los pesos de las interconexiones de las neuronas de manera automática hasta obtener buenas predicciones.

+ **Backpropagation:** Partiendo de la capa de salida, esa información de loss se propaga hacia todas las neuronas de la capa oculta que contribuyen directamente a la salida. Sin embargo las neuronas de la capa oculta solo reciben una fracción de la señal total de la loss, basándose aproximadamente en la contribución relativa que haya aportado cada neurona a la salida original.


# Algunos problemas en el entrenamiento de las redes neuronales

##

<div class="columns-2">

<div style="text-align: justify">

+ **Valores iniciales:** Se hace referencia a los valores que los pesos iniciales pueden tomar. Así, es recomendable llevar acabo una asignación de pesos pequeños generados de forma aleatoria.

$$w_{ij} \in (-0.5,0.5)$$

+ **Sobreajuste:** También denominado "overfitting", se produce cuando un sistema de aprendizaje automático se entrena demasiado o con datos anómalos, que hace que el algoritmo aprenda patrones que no son generales.

<center>

![](overfitting.png "Red neuronal"){height=180px}
<center>

<div style="text-align: justify">

+ **Escalado de las entradas:** Es preferible estandarizar todas las entradas para que tengan una media de cero y una desviación estándar de uno.

$$\tilde{x}_{i}=\displaystyle{\frac{(x_{i}-\bar{x}_{i})}{sd}}$$

+ **Número de capas y unidades ocultas:** El número de unidades ocultas está directamente relacionado con las capacidades de la red. En general, es mejor tener demasiadas unidades ocultas que muy pocas.

<center>
![](simple_profunda.png "Red neuronal"){width=450px}

<div class="columns-2">

<div/>

## Bibliografía

<div style="text-align: justify">

+ Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction. Springer Science & Business Media.

- Efron, B., & Hastie, T. (2016). Computer age statistical inference (Vol. 5). Cambridge University Press.


<div>

# ¡Gracias por tu atención!
